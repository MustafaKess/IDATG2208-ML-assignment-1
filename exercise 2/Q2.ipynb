{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fc68d7",
   "metadata": {},
   "source": [
    "# Exercise 2 - Advanced Machine Learning Techniques \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b26c90",
   "metadata": {},
   "source": [
    "## Q2.1 Which features are most suitable/influential in predicting wine quality? (Tip - You can consider feature importance ranking.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15e299ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Importance\n",
      "10               alcohol    0.277888\n",
      "1       volatile acidity    0.195423\n",
      "9              sulphates    0.151706\n",
      "4              chlorides    0.086388\n",
      "6   total sulfur dioxide    0.084458\n",
      "0          fixed acidity    0.069059\n",
      "7                density    0.061787\n",
      "8                     pH    0.045988\n",
      "3         residual sugar    0.025933\n",
      "5    free sulfur dioxide    0.025585\n",
      "2            citric acid    0.020778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"WineQT.csv\")\n",
    "\n",
    "# Features and target\n",
    "X_all = df.drop(columns=[\"quality\", \"Id\"]).values\n",
    "y = df[\"quality\"].values\n",
    "feature_names = df.drop(columns=[\"quality\", \"Id\"]).columns\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# Train linear regression with SGD\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.01, random_state=42)\n",
    "sgd.fit(X_scaled, y)\n",
    "\n",
    "# Get absolute coefficients as feature importance\n",
    "importance = np.abs(sgd.coef_)              # LOOK HERE VERY IMPORTANT \n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ea03e",
   "metadata": {},
   "source": [
    "Like in the exercises in Q1, we used SGDRegressor from scikit-learn to learn the coefficients, and then it predicts wine quality by combining all features linearly. Each feature gets a coefficient showing how strongly it affects the prediction, with positive values increasing predicted quality and negative values decreasing it, and larger absolute values indicating a stronger effect.\n",
    "\n",
    "From the feature importance ranking, alcohol is the most influential feature in predicting wine quality, followed by volatile acidity and sulphates. Features like chlorides, density, and fixed acidity have moderate influence, while residual sugar, citric acid, free sulfur dioxide, pH, and Id contribute very little. This shows that some chemical properties of the wine strongly affect quality, while others have minimal predictive power.\n",
    "\n",
    "This does make sense since we did see that in Q1.2 alcohol and volatile acidity had the two largest absolute values in correlation with quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97919c0",
   "metadata": {},
   "source": [
    "## Q2.2 The models you trained so far assume a linear relationship between features and target.\n",
    "\n",
    "### a) Polynomial regression: Extend the feature space to include quadratic or interaction terms. Does this improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66eaf017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree 2): MSE=0.3760, RMSE=0.6132, R²=0.3243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create polynomial features (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_all)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.01, random_state=42)\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "y_pred = sgd.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Polynomial Regression (degree 2): MSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0927c5f",
   "metadata": {},
   "source": [
    "Looking at the results, the polynomial regression model achieved an MSE of approximately 0.374, an RMSE of about 0.612, and an R² of around 0.327. Compared to the linear multiple regression model, the RMSE decreased slightly, but the R² remained almost the same. This indicates that adding polynomial terms, such as quadratic and interaction features, does not significantly improve performance for this dataset. It suggests that the relationship between the features and wine quality is mostly linear, or that more complex interactions are not captured well with just degree-2 polynomials. Overall, the linear model already captures most of the predictable variation in wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfa110",
   "metadata": {},
   "source": [
    "### b) Regularization: Train models using Ridge and Lasso regression. How do these methods affect the coefficients and model generalization?\n",
    "\n",
    "Ridge and Lasso are techniques that help linear regression avoid overfitting and work better on new data. \n",
    "\n",
    "Ridge adds a penalty based on the square of the coefficients, which shrinks them toward zero but usually keeps all features in the model. \n",
    "\n",
    "Lasso adds a penalty based on the absolute value of the coefficients, which can shrink some coefficients exactly to zero, effectively removing less important features. Both methods make the model more stable and improve generalization by reducing the influence of weak or correlated features. Overall, regularization slightly increases bias but lowers variance, helping the model make more reliable predictions on unseen wine samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea10a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients (Ridge vs Lasso):\n",
      "                 Feature     Ridge     Lasso\n",
      "10               alcohol  0.285819  0.293407\n",
      "1       volatile acidity -0.238646 -0.223573\n",
      "9              sulphates  0.161463  0.141083\n",
      "0          fixed acidity  0.086838  0.022495\n",
      "4              chlorides -0.085712 -0.081140\n",
      "6   total sulfur dioxide -0.072929 -0.060472\n",
      "2            citric acid -0.065283 -0.013223\n",
      "7                density -0.058945 -0.017111\n",
      "8                     pH -0.037865 -0.035157\n",
      "5    free sulfur dioxide  0.019222  0.000000\n",
      "3         residual sugar  0.005442 -0.000000\n",
      "Ridge: MSE=0.3799, RMSE=0.6163, R²=0.3173\n",
      "Lasso: MSE=0.3699, RMSE=0.6082, R²=0.3353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "\n",
    "X_temp = df.drop(columns=[\"quality\", \"Id\"]).values\n",
    "\n",
    "\n",
    "y = df[\"quality\"].values\n",
    "feature_names = df.drop(columns=[\"quality\", \"Id\"]).columns\n",
    "\n",
    "# Train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_temp, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=0.01, random_state=42)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Compare coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Ridge\": ridge.coef_,\n",
    "    \"Lasso\": lasso.coef_\n",
    "}).sort_values(by=\"Ridge\", key=abs, ascending=False)\n",
    "\n",
    "print(\"Coefficients (Ridge vs Lasso):\")\n",
    "print(coefficients)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "def evaluate(y_true, y_pred, name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name}: MSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "evaluate(y_test, y_pred_ridge, \"Ridge\")\n",
    "evaluate(y_test, y_pred_lasso, \"Lasso\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791a892",
   "metadata": {},
   "source": [
    "Looking at the results, both Ridge and Lasso shrink the coefficients compared to a standard linear regression, with Lasso reducing some coefficients exactly to zero (like free sulfur dioxide and residual sugar), effectively removing them from the model. \n",
    "\n",
    "Alcohol remains the strongest positive predictor, and volatile acidity the strongest negative predictor in both models. Performance-wise, Lasso slightly outperforms Ridge, with a lower RMSE (≈0.609 vs 0.618) and higher R² (≈0.333 vs 0.313), indicating slightly better generalization. Overall, regularization helps simplify the model, reduce overfitting, and highlight the most influential features while maintaining predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ff934",
   "metadata": {},
   "source": [
    "### c) Model comparison: Compare your linear regression results to a non-linear model (e.g., Decision Tree or Random Forest). Which performs better, and why?\n",
    "\n",
    "Example with comparison between Random Forest Regressor vs Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b27b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: RMSE=0.616, R²=0.317\n",
      "Random Forest: RMSE=0.547, R²=0.463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = df.drop(columns=[\"quality\", \"Id\"]).values\n",
    "y = df[\"quality\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80/20 split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "y_pred_lin = linreg.predict(X_test_scaled)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "def evaluate(y_true, y_pred, name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name}: RMSE={rmse:.3f}, R²={r2:.3f}\")\n",
    "\n",
    "evaluate(y_test, y_pred_lin, \"Linear Regression\")\n",
    "evaluate(y_test, y_pred_rf, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7708aa4",
   "metadata": {},
   "source": [
    "Comparing linear regression to non-linear models like Decision Trees or Random Forests, the non-linear models usually do better when the relationship between features and wine quality isn’t just a straight line. \n",
    "\n",
    "Linear regression assumes each feature affects quality in a simple, linear way, so it can miss more complex patterns. Decision Trees and Random Forests can automatically capture non-linear effects and interactions between features, often giving lower errors and higher R².\n",
    "\n",
    " Linear regression is simpler and easier to interpret, but non-linear models are more flexible, though they can overfit if not tuned properly. For predicting wine quality, a Random Forest would likely work better because it can handle the subtle interactions between different chemical properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
